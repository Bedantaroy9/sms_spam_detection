# -*- coding: utf-8 -*-
"""sms_spam_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mqi1lJ1tqIH0DgQfRTr3nN_RdCPR2wWq
"""

import numpy as np
import pandas as pd

from google.colab import files

uploaded = files.upload()

import os

print(os.listdir())

import pandas as pd

df = pd.read_csv('spam.csv', encoding='latin1')

df.head()

df.shape

"""# Data Cleaning"""

df.info()

df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)

df.head()

df.rename(columns={'v1':'target','v2':'text'},inplace=True)
df.head()

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['target'] = encoder.fit_transform(df['target'])

df.head()

df.isnull().sum() #check missing values

# duplicated values
df.duplicated().sum()

df = df.drop_duplicates(keep='first') #removed duplicates

df.duplicated().sum()

df.shape

"""# EDA"""

df['target'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(), labels=['ham', 'spam'], autopct="%0.2f")
plt.show()

import nltk

!pip install nltk

nltk.download('punkt')

df['num_character'] = df['text'].apply(len)

df.head()

from nltk.tokenize import wordpunct_tokenize

df['num_words'] = df['text'].apply(lambda x: len(wordpunct_tokenize(x)))

df.head()

import re

# Split on punctuation followed by space (simple sentence splitter)
df['num_sentences'] = df['text'].apply(lambda x: len(re.split(r'[.!?]+\s', x.strip())))

df.head()

df[['num_character','num_words','num_sentences']].describe()

import seaborn as sns

sns.histplot(df[df['target'] == 0]['num_character'])
sns.histplot(df[df['target'] == 1]['num_character'],color='red')

sns.pairplot(df,hue='target')

df.head()

numeric_df = df.select_dtypes(include=['number'])
corr_matrix = numeric_df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

"""# Data Processing
##lower case
##Tokenizer
##removing special character
##removing stop words and punctuation
##stemming

"""

import re
from nltk.stem import SnowballStemmer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

stemmer = SnowballStemmer('english')

def transform_text(text):
    # Lowercase
    text = text.lower()

    # Keep only words and numbers
    tokens = re.findall(r'\b\w+\b', text)

    # Remove stopwords
    filtered = [word for word in tokens if word not in ENGLISH_STOP_WORDS]

    # Apply stemming
    stemmed = [stemmer.stem(word) for word in filtered]

    return " ".join(stemmed)

transform_text('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')

df['text'][0]

df['transformed text'] = df['text'].apply(transform_text)

df.head()

spam_corpus = []                   ### it gives us the transformed words

for msg in df[df['target'] == 1]['transformed text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)

len(spam_corpus)

from collections import Counter
Counter(spam_corpus).most_common(30)  ## top 30 most frequency words

df.head()

"""# Model building"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
cv = CountVectorizer()
tf = TfidfVectorizer(max_features=3000)

x = tf.fit_transform(df['transformed text']).toarray()

x.shape

# from sklearn.preprocessing import MinMaxScaler
# scaler = MinMaxScaler()
# x = scaler.fit_transform(x)

y = df['target'].values

y
y.shape

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.2, random_state=2)

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

gnb.fit(X_train, Y_train)
y_pred = gnb.predict(X_test)
print(accuracy_score(Y_test, y_pred))
print(confusion_matrix(Y_test, y_pred))
print(precision_score(Y_test, y_pred))

mnb.fit(X_train, Y_train)
y_pred = mnb.predict(X_test)
print(accuracy_score(Y_test, y_pred))
print(confusion_matrix(Y_test, y_pred))
print(precision_score(Y_test, y_pred))

bnb.fit(X_train, Y_train)
y_pred = bnb.predict(X_test)
print(accuracy_score(Y_test, y_pred))
print(confusion_matrix(Y_test, y_pred))
print(precision_score(Y_test, y_pred))

"""### we are taking multinomialNB"""

import pickle
pickle.dump(tf, open('vectorizer.pkl','wb'))
pickle.dump(mnb, open('model.pkl','wb'))

